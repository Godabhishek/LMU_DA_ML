{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most features in our data are just numbers without units attached to them. However, giving a length in cm or inch (or in [Newton seconds vs pound-force seconds][1]) will give different values and thus different outputs when feeding these numbers e.g. to a neural network. Even more importantly, the values of different features may naturally cover completely different ranges. This may have a large effect on the performance of machine learning models:\n",
    "* obviously e.g. when computing the distance of nearest neighbors in the kNN estimator\n",
    "* but also if a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly.\n",
    "\n",
    "Some algorithms will be more susceptible to the scaling of the data than others:\n",
    "* Neural networks expect all input features to vary in a similar way, and ideally to look like standard normally distributed data, i.e. Gaussian with zero mean and unit variance.\n",
    "* BDTs that just apply `if` statements on the features are typically very robust.\n",
    "\n",
    "The `sklean` documentation provides further information on [preprocessing of data][2].\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Mars_Climate_Orbiter#Cause_of_failure\n",
    "[2]: https://scikit-learn.org/stable/modules/preprocessing.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example I\n",
    "One example that illustrates the importance of preprocessing the data to rescale feature values to \"standard ranges\" is the application of SVMs to the `sklearn`'s cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use here a *linear support vector machine*, a common linear classification model, implemented in `LinearSVC` (support vector classifier). SVC try to find a (hyper-) plane in phase space that optimally separates the two given populations. \n",
    "* Predictions are then made by checking on which side of this plane new samples lie. \n",
    "* The (hyper-) plane is defined by only a few of the training samples (the supporting vectors of features); those that lie close to the resulting plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SVC and load data\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features in this dataset have very different ranges as illustrated by the following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot features\n",
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot(cancer.data, whis = \"range\")\n",
    "plt.xlabel(\"feature\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit data without rescaling\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now rescale the features and retrain\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# compute minimum and maximum on the training data\n",
    "# note that we must train the scaler only on the training data (but not the full dataset)\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "# rescale the training data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "# retrain\n",
    "svm.fit(X_train_scaled, y_train).score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same effect (although much smaller) e.g. on MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=1000, random_state=0)\n",
    "print \"Score using unscaled data:\", mlp.fit(X_train, y_train).score(X_test, y_test)\n",
    "print \"Score using rescaled data:\", mlp.fit(X_train_scaled, y_train).score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no effect e.g. on BDT\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "print \"Score using unscaled data:\", dt.fit(X_train, y_train).score(X_test, y_test)\n",
    "print \"Score using rescaled data:\", dt.fit(X_train_scaled, y_train).score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large values of features may also lead to convergence issues. \n",
    "(Note that many linear models include regularization terms that are introduced to avoid very large values of the parameters.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(85)\n",
    "nsamples = 50\n",
    "pops     = 2\n",
    "yshift   = 0.5\n",
    "yscale   = 100\n",
    "noise    = 0.3\n",
    "\n",
    "### helpers\n",
    "def MakeXYcorrSample(x0 = 0, y0 = 0, noise = noise, yscale = yscale, pops = pops, yshift = yshift):\n",
    "  # space\n",
    "  X = np.zeros((nsamples * pops, 2))\n",
    "  y = np.zeros(nsamples * pops)\n",
    "  # fill\n",
    "  X[:, 0] = x0 + np.random.rand(nsamples * pops)\n",
    "  for n in range(pops):\n",
    "    X[n*nsamples:(n+1)*nsamples, 1] = yscale * (y0 + X[n*nsamples:(n+1)*nsamples, 0] - noise/2. + noise * np.random.rand(nsamples) + n*yshift)\n",
    "    y[n*nsamples:(n+1)*nsamples] = n\n",
    "  return X, y\n",
    "\n",
    "### make and plot dataset\n",
    "X, y = MakeXYcorrSample()\n",
    "for pop in range(pops):\n",
    "  plt.scatter(*X[y == pop,0:2].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot: We have two populations that are obviously easy to separate? Or are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepare data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "### fit\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC().fit(X_train, y_train)\n",
    "\n",
    "print \"Score on training data:\", model.score(X_train, y_train)\n",
    "print \"Score on validation data:\", model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score on the training data shows that we cannot even model the training data correctly using the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualize\n",
    "for pop in range(pops):\n",
    "  plt.scatter(*X[y == pop,0:2].T)\n",
    "if \"coef_\" in dir(model):\n",
    "  eps = 0.1\n",
    "  x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n",
    "  y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps  \n",
    "  xx = np.linspace(x_min, x_max, 1000)\n",
    "  yy = np.linspace(y_min, y_max, 1000)\n",
    "  X1, X2 = np.meshgrid(xx, yy)\n",
    "  X_grid = np.c_[X1.ravel(), X2.ravel()]\n",
    "  decision_values = model.predict(X_grid)\n",
    "  from matplotlib.colors import ListedColormap\n",
    "  plt.gca().imshow(decision_values.reshape(X1.shape), extent=(x_min, x_max, y_min, y_max), aspect='auto', origin='lower', alpha=0.5, cmap = ListedColormap(['#0000aa', '#ff2020', '#50ff50']))\n",
    "  #\n",
    "  line = np.linspace(0, 1)\n",
    "  for coef, intercept in zip(model.coef_, model.intercept_):\n",
    "    ax = plt.plot(line, -(line * coef[0] + intercept) / coef[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
