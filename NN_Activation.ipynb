{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and terms\n",
    "\n",
    "As we saw from the logistic regression yesterday, linear classifiers are often not the best at solving complicated problems. Neural networks introduce nonlinearity. They were originally developed as mathematical models of the information processing capabilities of biological brains, and were popular in the 80s and early 90s. Recently they have become popular again, especially as deep neural networks DNNs, including convolutional NNs (CNN), recurrent NNs (RNN), etc. Those are beyond the scope of this class, but we will introduce the basics of NNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a diagram of a simple NN (a MLP, multilayer perceptron, with a single hidden layer):\n",
    "![NNFig](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A NN is made up of _neurons_ that receive multiple inputs, process them, and send the output to the neurons in the following layer. Here, the network is _dense_; every node of the hidden layer is connected to all previous and following nodes. \n",
    "The arrows indicate that information flows from left (the input) to the right (the output). Every arrow has an assigned weight that is a free parameter in the training of the network. \n",
    "\n",
    "Computing a series of weighted sums as is done by the above network is mathematically the same as computing just one weighted sum. An MLP with multiple linear hidden layers that all just sum up the inputs is thus equivalent to an MLP with a single linear hidden layer. To make this model more powerful than a linear model, a non-linear function, the _activation function_, is applied to the weighted sum for each neuron in the hidden layer and used to determine the output that is propagated as input to the following neurons.\n",
    "\n",
    "The number of neurons in the output layer and the choice of _output activation function_ depend on the task the network is intended for. For binary classification tasks, a typical setup has a single output neuron with a logistic sigmoid activation.\n",
    "Large neural networks made up of many of hidden layers of computation go under the term _deep learning_. Note that for dense layers, the number of parameters increases linearly in the number of hidden layers but quadratically in the number of neurons in each layer.\n",
    "\n",
    "In order to train the NN, we have to determine the weight matrix $\\Theta$ that minimizes the _cost function_. \n",
    "This is done using a method called _backpropagation_.\n",
    "The cost function of a NN is similar to what we have for logistic regression, modified to take into account possible multiple outputs, and with more complicated regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of the mathematics behind NNs\n",
    "* One can write a neuron's output as $a_i = g\\left(\\theta_i^T x_i\\right)$, where $i$ is the index of the neuron, $x_i$ its vector of inputs, and $\\theta_i$ are the weights of the input connections of the neuron (also a vector). $g$ is the activation fuction. \n",
    "* The NN above has an input layer (layer 1), a single hidden layer (layer 2), and an output layer (layer 3). One can have more hidden layers. \n",
    "* Combining the outputs of all neurons in one layer into one vector $z$, we write $z^{(j)} = \\Theta^{(j-1)}a^{(j-1)}$ with an upper index to label the layer. The matrix $\\Theta^{(j-1)}$ formed from all weights of all neurons in layer $j-1$.\n",
    "* Then $a^{(j)} = g(z^{(j)})$. Thus evaluating the NN is a series of matrix multiplications followed by activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "![Neuron][2]\n",
    "\n",
    "To train the network, the cost function is minimized by propagating \"errors\" in the outputs $z_i^{(j)}$ backwards. \n",
    "The partial derivatives of the cost function with respect to the weights are computed,\n",
    "$$\\frac{\\partial E}{\\partial w_{ij}} = \\frac{\\partial E}{\\partial o_j} \\frac{\\partial o_j}{\\partial\\text{net}_j} \\frac{\\partial \\text{net}_j}{\\partial w_{ij}},$$\n",
    "and the weights are modified such that the cost function is reduced:\n",
    "$$ \\Delta w_{ij} = - \\eta \\frac{\\partial E}{\\partial w_{ij}}.$$ <!--= - \\eta \\delta_j o_i -->\n",
    "$\\eta$ is a metaparameter in the training that determines the _learning rate_.\n",
    "Usually something like (stochastic) gradient descent is used to solve this problem. \n",
    "For more details on backpropagation, look, for example, at the [ML course][1] mentioned above.\n",
    "\n",
    "[1]: https://www.coursera.org/learn/machine-learning\n",
    "[2]: https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/ArtificialNeuronModel_english.png/640px-ArtificialNeuronModel_english.png \"a single neuron of an artifical network\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "The activation function of a neuron transforms the net input into the activation (output) of the neuron, i.e. whether the neutron is firing or not.\n",
    "\n",
    "Popular choices include ([source][2]):\n",
    "![1]\n",
    "<!--\n",
    "* For example, if we use a logistic function as the activation function, we can have $g\\left(\\theta^Tx\\right) = \\frac{1}{1+\\mathrm{exp}\\left(-\\theta^Tx\\right)}$, \n",
    "* or if a Rectified Linear Unit (ReLU), $g\\left(\\theta^Tx\\right) = \\mathrm{max}\\left(0, \\theta^Tx\\right)$. \n",
    "-->\n",
    "[1]: figures/activation_functions.png \"overview of commonly used activation functions\"\n",
    "[2]: https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Keras][4] provides a large set of activation functions: sigmoid, tanh, relu (rectified linear unit), linear, [elu][3] (exponential linear unit), [softmax][5], ...\n",
    "* In addition, e.g. learnable activations (which maintain a state) are available as advanced activation layers. These include `PReLU` and `LeakyReLU`.\n",
    "* Differentiable functions allow the network to be trained with gradient descent.\n",
    "* Activation functions map the (potentially unbounded) input range of the net input of the neurons to a finite output range and are therefore sometimes  referred to as squashing functions.\n",
    "\n",
    "[3]: https://arxiv.org/pdf/1511.07289.pdf\n",
    "[4]: https://keras.io/activations/\n",
    "[5]: https://de.wikipedia.org/wiki/Softmax-Funktion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
