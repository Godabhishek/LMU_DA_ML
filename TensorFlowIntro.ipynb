{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "\n",
    "We don't have to code up back propagation for every possible function or neural network architecture that we want to fit. There are lot's of libraries targetted towards machine learning that make this task easy and computationally efficient. One of the most popular libraries is [TensorFlow](https://www.tensorflow.org/). It was developed by Google Brain and is now open source under the Apache License 2.0.\n",
    "\n",
    "The workflow consists of building a computational graph where each \"operations\" acting on \"Tensors\" that can be automatically differentiated. The Tensors themselves don't hold values, but instead are \"initialized\" or \"fed\" when actually running the computation. We will see how that works in this tutorial.\n",
    "\n",
    "The TensorFlow website contains a much more [detailed Introduction](https://www.tensorflow.org/guide/low_level_intro) if you want to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually build a NN in TensorFlow\n",
    "\n",
    "Let's build a 1-hidden-layer NN, similar to what we did in [NNFromScratch.ipynb](NNFromScratch.ipynb) now with TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholders are used for values that should be fed in later. Dimensions of size `None` are meant to be of arbitray size, in this case this will be the training example index. \n",
    "\n",
    "When we create Tensors, they will be added to the current graph of TensorFlow. To identify them later, in case we haven't assigned them to a python variable it is useful to give them a name. TensorFlow will attach an index to the name if it is already taken.\n",
    "\n",
    "The first Tensor will hold the input values that we will feed in later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = tf.placeholder(tf.float32, (None, 2), name='input')\n",
    "inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the weights an biases for the hidden layer. We could use a placeholder as well and feed it the inital weights, but to illustrate a different concept, lets use a variable. We use the `tf.get_variable` method. Note that this method won't add indices to the names when they are already taken, but throw an exception instead.\n",
    "\n",
    "Variables have to be initialized. We could e.g. specify that the weights are initalized to some fixed values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.get_variable(\"W\", dtype=tf.float32, initializer=np.random.randn(2, 16).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.get_variable(\"b\", dtype=tf.float32, initializer=tf.zeros(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'W:0' shape=(2, 16) dtype=float32_ref>\n",
      "<tf.Variable 'b:0' shape=(16,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No lets define the output of the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'z:0' shape=(?, 16) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tf.add(tf.matmul(inp, W), b, name=\"z\")\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'a:0' shape=(?, 16) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.nn.relu(z, name=\"a\")\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the weights and outputs of the output layer.\n",
    "\n",
    "Let's try another method for variable initialization here - using tensorflows initializers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'W2:0' shape=(16, 1) dtype=float32_ref>\n",
      "<tf.Variable 'b2:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "W2 = tf.get_variable(\"W2\", dtype=tf.float32, initializer=tf.random_normal_initializer()(shape=(16, 1)))\n",
    "b2 = tf.get_variable(\"b2\", dtype=tf.float32, initializer=tf.zeros(1))\n",
    "print(W2)\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Add:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2 = tf.add(tf.matmul(a, W2), b2)\n",
    "z2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will skip the activation function, since we will use a Loss function that already applies the sigmoid transformation. This is numerically more stable.\n",
    "\n",
    "But first, we need to define a Tensor that will hold the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32, (None,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the binary cross entropy with a sigmoid transformation of the input values that don't have the sigmoid applied already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0825 19:06:20.646105 140337554827008 deprecation.py:323] From /home/n/Nikolai.Hartmann/conda/mlkurs/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'logistic_loss:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=z2)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now - **and this is the whole point of this tutorial** - to get the gradients w.r.t. all parameters, we can simply create the gradients as a `tf` operation and done!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'gradients/MatMul_grad/MatMul_1:0' shape=(2, 16) dtype=float32>,\n",
       " <tf.Tensor 'gradients/z_grad/Reshape_1:0' shape=(16,) dtype=float32>,\n",
       " <tf.Tensor 'gradients/MatMul_1_grad/MatMul_1:0' shape=(16, 1) dtype=float32>,\n",
       " <tf.Tensor 'gradients/Add_grad/Reshape_1:0' shape=(1,) dtype=float32>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = tf.gradients(L, [W, b, W2, b2])\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the NN and calculate the gradient\n",
    "\n",
    "Getting the output of the NN and the gradient w.r.t. the parameters is now simply a matter of feeding in the values and running the actual `tf` graph. Lets create the feed values with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2877745 , -0.45729127],\n",
       "       [-0.05789639,  1.69953029],\n",
       "       [ 1.52304573,  0.49659072],\n",
       "       [-2.54455919,  1.03957212],\n",
       "       [ 0.94309014,  0.82602736],\n",
       "       [-1.39240318, -2.55734321],\n",
       "       [ 0.23819814,  0.34620872],\n",
       "       [ 0.82564916,  0.27619472],\n",
       "       [ 1.3555375 , -0.08659935],\n",
       "       [-0.25835985,  0.05114919]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_val = np.random.randn(10, 2)\n",
    "inp_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val = np.random.randint(0, 2, size=10).reshape(-1, 1)\n",
    "y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the graph, we have to create a `tf.Session` and pass a Tensor to the `run` method and a dictionary with the values for all placeholder tensors - in our case the input and targets. But before that, the values of the variables have to be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets just output the values of the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 2.8025308 , -3.071056  , -3.1624866 , -0.2209892 ,  0.6038144 ,\n",
       "         -3.3342512 , -2.829316  ,  1.104261  , -0.40069878,  0.52425086,\n",
       "          0.8893442 ,  0.03627827,  2.3906946 ,  1.065886  , -0.00704979,\n",
       "          1.1612515 ],\n",
       "        [-0.7958876 ,  0.38841072, -1.244313  ,  0.44525224,  0.19730797,\n",
       "          0.42169818, -1.009977  , -0.17122361, -4.085935  , -0.84847534,\n",
       "          2.8433928 , -0.05871465, -0.3023622 ,  3.0012853 , -0.01985054,\n",
       "          0.37946135]], dtype=float32),\n",
       " array([-1.1988204 ,  0.40968737,  1.984129  ,  0.19213468, -0.35252774,\n",
       "         0.4447983 ,  1.8328683 , -0.35094273,  2.689114  , -0.8303844 ,\n",
       "        -1.1616665 , -0.05746276, -0.31892526, -0.9777993 ,  0.00646718,\n",
       "        -0.6779789 ], dtype=float32),\n",
       " array([[ -2.0471401 ],\n",
       "        [ -3.6832035 ],\n",
       "        [ -3.1061494 ],\n",
       "        [  0.07250378],\n",
       "        [ -6.486447  ],\n",
       "        [ -0.96950436],\n",
       "        [-11.957878  ],\n",
       "        [  0.5033768 ],\n",
       "        [ -2.3783982 ],\n",
       "        [ -0.6327585 ],\n",
       "        [ -0.8005719 ],\n",
       "        [ -0.8803862 ],\n",
       "        [ -0.7928982 ],\n",
       "        [ -1.1062554 ],\n",
       "        [ -2.0641184 ],\n",
       "        [ -5.6679816 ]], dtype=float32),\n",
       " array([-2.1266787], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_dict = {inp : inp_val, y : y_val}\n",
    "grad_val = sess.run(grad, feed_dict=feed_dict)\n",
    "grad_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And store them for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW_tf, db_tf, dW2_tf, db2_tf = grad_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can reproduce that with the formulas we used in [NNFromScratch.ipynb](NNFromScratch.ipynb)\n",
    "\n",
    "Here a copy paste of the relevant functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    dZ = (Z >= 0)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    Z_curr = np.matmul(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    return activation_func(Z_curr), Z_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        derivative_activation_func = relu_derivative\n",
    "    elif activation is \"sigmoid\":\n",
    "        derivative_activation_func = sigmoid_derivative\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "            \n",
    "    dZ_curr = dA_curr * derivative_activation_func(Z_curr)\n",
    "    dW_curr = np.matmul(\n",
    "        dZ_curr,\n",
    "        # need to transpose only the last 2 dimensions, \n",
    "        # since the first dimension is the training example index\n",
    "        np.transpose(A_prev, (0, 2, 1))\n",
    "    )\n",
    "    db_curr = dZ_curr\n",
    "    dA_prev = np.matmul(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_value(Y_hat, Y):\n",
    "    return - np.mean(Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_derivative(Y_hat, Y):\n",
    "    return - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's calculate the forward pass. Unfortunately, tf and numpy have a bit different conventions for matmul, so i'm very sorry for all the confusing transposes and reshapes in the following section. If you have to do something like that, usually it's best to experiment and see if output dimensions are correct after each step.\n",
    "\n",
    "First, lets store the initialized values of the NN parameters in python variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_val, b_val, W2_val, b2_val = sess.run([W, b, W2, b2], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then run our manual `numpy` forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.13885445 0.\n",
      " 0.         0.         0.50344369 0.65172233 0.23873353 0.90670911\n",
      " 0.         0.60770566 0.86829198 0.09452994]\n",
      "[-0.39402405 -1.46525636 -0.27215844 -0.03547932  0.13885445 -0.34297248\n",
      " -0.62451726 -0.54651447  0.50344369  0.65172233  0.23873353  0.90670911\n",
      " -0.27551562  0.60770566  0.86829198  0.09452994]\n"
     ]
    }
   ],
   "source": [
    "a_val, z_val = single_layer_forward_propagation(inp_val.reshape(-1, 2, 1), W_val.T, b_val.reshape(-1, 1))\n",
    "print(a_val[0].reshape(-1))\n",
    "print(z_val[0].reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compared to what `tf` gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.         -0.         -0.         -0.          0.13885446 -0.\n",
      " -0.         -0.          0.5034437   0.6517223   0.23873353  0.90670913\n",
      " -0.          0.60770565  0.868292    0.09452995]\n",
      "[-0.39402404 -1.4652563  -0.27215844 -0.03547932  0.13885446 -0.3429725\n",
      " -0.62451726 -0.5465145   0.5034437   0.6517223   0.23873353  0.90670913\n",
      " -0.27551562  0.60770565  0.868292    0.09452995]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(a, feed_dict=feed_dict)[0])\n",
    "print(sess.run(z, feed_dict=feed_dict)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.224269   0.08674905 0.21573556 0.00045921 0.17649171 0.00893797\n",
      " 0.37715723 0.32941938 0.11585327 0.33824911]\n",
      "[-1.24095959 -2.35399121 -1.29069288 -7.68554703 -1.54029973 -4.7084686\n",
      " -0.50163194 -0.71081228 -2.0322986  -0.67110654]\n"
     ]
    }
   ],
   "source": [
    "a2_val, z2_val = single_layer_forward_propagation(\n",
    "    a_val.reshape(-1, 16, 1), W2_val.T, b2_val.reshape(-1, 1), activation=\"sigmoid\"\n",
    ")\n",
    "print(a2_val.reshape(-1))\n",
    "print(z2_val.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `tf`, we don't have `a2` because we used a definition of the loss function where the sigmoid activation is already included. But we have `z2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2409596  -2.353991   -1.2906929  -7.685547   -1.5402997  -4.7084684\n",
      " -0.501632   -0.71081245 -2.0322988  -0.6711066 ]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(z2, feed_dict=feed_dict).reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we implemented the forward pass correctly, so now lets do the backward pass and check if we get the same gradients like Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.28910666e+00,  1.09498928e+00,  1.27508012e+00, -2.17766034e+03,\n",
       "        1.21431685e+00, -1.11882225e+02,  1.60554165e+00,  1.49124501e+00,\n",
       "       -8.63160824e+00, -2.95640096e+00])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dL = get_loss_derivative(a2_val, y_val.reshape(-1, 1, 1))\n",
    "dL.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propagate back into the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.04714007  -3.68320363  -3.10614896   0.07250378  -6.48644681\n",
      "  -0.96950433 -11.95787755   0.50337676  -2.37839827  -0.63275843\n",
      "  -0.80057187  -0.880386    -0.79289815  -1.10625534  -2.06411813\n",
      "  -5.66798132]\n",
      "[-2.12667851]\n"
     ]
    }
   ],
   "source": [
    "da, dW2, db2 = single_layer_backward_propagation(\n",
    "    dL, W2_val.T, b2_val.reshape(-1, 1), z2_val, a_val, activation=\"sigmoid\"\n",
    ")\n",
    "print(np.sum(dW2, axis=0).reshape(-1))\n",
    "print(np.sum(db2, axis=0).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.0471401   -3.6832035   -3.1061494    0.07250378  -6.486447\n",
      "  -0.96950436 -11.957878     0.5033768   -2.3783982   -0.6327585\n",
      "  -0.8005719   -0.8803862   -0.7928982   -1.1062554   -2.0641184\n",
      "  -5.6679816 ]\n",
      "[-2.1266787]\n"
     ]
    }
   ],
   "source": [
    "print(dW2_tf.reshape(-1))\n",
    "print(db2_tf.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from there into the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.80253083 -3.07105562 -3.16248658 -0.22098917  0.60381441 -3.33425114\n",
      "  -2.8293158   1.10426104 -0.40069876  0.52425093  0.88934434  0.03627826\n",
      "   2.39069446  1.06588626 -0.00704979  1.16125159]\n",
      " [-0.79588766  0.38841069 -1.24431291  0.44525222  0.19730798  0.42169825\n",
      "  -1.00997679 -0.17122365 -4.08593471 -0.84847531  2.84339269 -0.05871465\n",
      "  -0.30236225  3.00128546 -0.01985054  0.37946132]]\n",
      "[-1.19882035  0.40968728  1.98412887  0.19213473 -0.35252772  0.44479829\n",
      "  1.83286822 -0.35094271  2.68911412 -0.83038462 -1.16166641 -0.05746277\n",
      " -0.31892523 -0.9777993   0.00646718 -0.67797882]\n"
     ]
    }
   ],
   "source": [
    "dinp, dW, db = single_layer_backward_propagation(\n",
    "    da, W_val.T, b_val.reshape(-1, 1), z_val, inp_val.reshape(-1, 2, 1), activation=\"relu\"\n",
    ")\n",
    "print(np.sum(dW, axis=0).T)\n",
    "print(np.sum(db, axis=0).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.8025308  -3.071056   -3.1624866  -0.2209892   0.6038144  -3.3342512\n",
      "  -2.829316    1.104261   -0.40069878  0.52425086  0.8893442   0.03627827\n",
      "   2.3906946   1.065886   -0.00704979  1.1612515 ]\n",
      " [-0.7958876   0.38841072 -1.244313    0.44525224  0.19730797  0.42169818\n",
      "  -1.009977   -0.17122361 -4.085935   -0.84847534  2.8433928  -0.05871465\n",
      "  -0.3023622   3.0012853  -0.01985054  0.37946135]]\n",
      "[-1.1988204   0.40968737  1.984129    0.19213468 -0.35252774  0.4447983\n",
      "  1.8328683  -0.35094273  2.689114   -0.8303844  -1.1616665  -0.05746276\n",
      " -0.31892526 -0.9777993   0.00646718 -0.6779789 ]\n"
     ]
    }
   ],
   "source": [
    "print(dW_tf)\n",
    "print(db_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Tensorflow does the same thing we attempted to do before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
